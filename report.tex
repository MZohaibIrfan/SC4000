\documentclass[11pt]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath,amssymb}
\usepackage{url}
\usepackage{enumitem}

\title{Google -- Fast or Slow? Predict AI Model Runtime\\
\large Tile-Only Baseline: Data Understanding and Proposed Solution}
\author{Muhammad Zohaib Irfan}
\date{\today}

\begin{document}
\maketitle

\section{Competition Overview}

The competition ``Google -- Fast or Slow? Predict AI Model Runtime'' requires predicting which compilation configuration makes a TPU program execute fastest. Submissions rank the configuration indices for each program by predicted runtime (lower is better). In this phase, we focus \textbf{strictly on tile-level data} and defer layout to later work.

The course report requires describing the competition, challenges, methodology, experimental study, and leaderboard evidence. Direct reuse of released solutions is not allowed; referenced ideas must be acknowledged.

\section{Tile Dataset: \texttt{.npz} Format}

Each \texttt{.npz} file represents a single compiled graph with $n$ nodes and $m$ edges, compiled under $c$ configurations (graph-level). The dataset contains the following keys:

\begin{itemize}[leftmargin=*]
  \item \texttt{node\_feat} $\in \mathbb{R}^{n\times 140}$: float32 node features (topologically ordered)
  \item \texttt{node\_opcode} $\in \mathbb{Z}^{n}$: opcode (int32) for each node
  \item \texttt{edge\_index} $\in \mathbb{Z}^{m\times 2}$: directed edges $[u,v]$ meaning $u \leftarrow v$ (consumer $\leftarrow$ producer)
  \item \texttt{config\_feat} $\in \mathbb{R}^{c\times 24}$: per-configuration features
  \item \texttt{config\_runtime}, \texttt{config\_runtime\_normalizers} $\in \mathbb{Z}^{c}$: runtime (ns) and baseline normalizer per config
\end{itemize}

\noindent
We predict the best configuration by minimizing:
\[
r_{ij}^{\mathrm{norm}} = \frac{\texttt{runtime}_{ij}}{\texttt{normalizer}_{ij}}.
\]

\section{Feature Engineering (Tile Only)}

For each program $i$ and configuration $j$, we construct a feature vector:
\[
x_{ij}
=
\underbrace{\texttt{config\_feat}_{ij}}_{\in\mathbb{R}^{24}}
\ \oplus\
\underbrace{\phi(\texttt{node\_feat}, \texttt{node\_opcode}, \texttt{edge\_index})}_{\in\mathbb{R}^{8}}
\in \mathbb{R}^{32},
\]
where $\phi$ extracts graph-level summary statistics:
\begin{enumerate}[leftmargin=*]
  \item Node count $n$ and feature dimension $F=140$
  \item Global mean and standard deviation across all node features
  \item Opcode unique count and total count
  \item Edge count $m$ and density proxy $m/\max(1,n^2)$
\end{enumerate}


\subsection*{Feature Standardization}
Training-time standardization uses train-only statistics to prevent data leakage:
\[
x' = \frac{x - \mu_{\text{train}}}{\sigma_{\text{train}} + \varepsilon}.
\]

\section{Model Architecture}

\subsection{Why an MLP?}
Tile representations are tabular and low-dimensional after summarization. A multi-layer perceptron offers sufficient expressiveness without requiring graph message passing.

\subsection{Network Definition}
Let $x \in \mathbb{R}^{32}$. Our model consists of three layers:
\begin{align*}
h_1 &= \mathrm{ReLU}(W_1 x + b_1), \quad W_1 \in \mathbb{R}^{256\times 32}, \\
h_1 &= \mathrm{LayerNorm}(h_1),\\
h_2 &= \mathrm{ReLU}\big(W_2\,\mathrm{Dropout}(h_1;0.1) + b_2\big),
\quad W_2 \in \mathbb{R}^{128\times 256}, \\
h_2 &= \mathrm{LayerNorm}(h_2),\\
\hat{y} &= W_3\,\mathrm{Dropout}(h_2;0.1) + b_3,
\quad W_3 \in \mathbb{R}^{1\times 128}.
\end{align*}

\subsection{Design Choices}

\paragraph{LayerNorm:} Runtime features vary significantly across programs. LayerNorm stabilizes hidden activations, improving training convergence for large-batch MLPs.

\paragraph{Dropout:} The tile dataset is large, but many configuration features are correlated. Dropout (rate 0.1) mitigates co-adaptation and improves generalization to unseen graphs.

\section{Training Setup}

\begin{itemize}[leftmargin=*]
  \item \textbf{Optimizer:} AdamW with learning rate $\text{lr}=10^{-3}$ and weight decay $\text{wd}=10^{-5}$
  \item \textbf{Batch size:} 131,072
  \item \textbf{Epochs:} 5
  \item \textbf{Random seeds:} \{42, 43, 44\}
\end{itemize}

\subsection{Log-Target Regression}
Raw runtimes vary by orders of magnitude, leading to unstable gradients in direct regression. We predict:
\[
y = \log(r_{ij}^{\text{norm}}).
\]
This transformation stabilizes variance, reduces the dominance of extreme runtimes, and empirically improves regret and ranking metrics.

\subsection{Ensembling}
For each seed $s$, model $f_{\theta_s}$ yields prediction $\hat{y}_s$. The final prediction averages across seeds:
\[
\hat{y} = \frac{1}{3}\sum_{s \in \{42,43,44\}} \hat{y}_s.
\]
Ensembling reduces variance, mitigates seed sensitivity, and improves ranking stability.

\subsection{Per-Program Calibration}
Raw predicted magnitudes differ across programs, but only \emph{relative ordering within each file} matters for ranking. We apply per-program standardization:
\[
\hat{y}_{ij}' =
\frac{\hat{y}_{ij} - \overline{\hat{y}}_{i\cdot}}
{\mathrm{std}(\hat{y}_{i\cdot}) + 10^{-12}}.
\]
This enforces consistent scaling across programs and empirically improves Acc@1.

\section{Results}

\subsection{Validation Performance}
The model achieves the following metrics on the validation set:

\begin{center}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Acc@1 & 0.0814 \\
Avg Regret & 0.252924 \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Training Dynamics}
Averaged across all three random seeds, the model exhibits stable convergence:
\begin{itemize}[leftmargin=*]
  \item \textbf{Train loss:} $0.45 \rightarrow 0.21$
  \item \textbf{Valid MSE:} $0.36 \rightarrow 0.32$
\end{itemize}

\subsection{Ablation Study: Model Depth}
We experimented with deeper architectures (4--6 layers). However, these models consistently achieved worse performance on the leaderboard compared to the 3-layer baseline. We therefore retain the 3-layer MLP as our final architecture.

\end{document}